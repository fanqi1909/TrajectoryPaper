\section{Responses to Reviewer 3}
\emph{Though the implementation references Spark as the implementation
platform (as it also reflects in github repo provided), the algorithm design is
mostly limited to MapReduce, aka only Hadoop, which is a very small subset of
Spark. This may have a negative impact on the baseline implementation.
Particularly, recent releases of Spark have introduced window functions that can
be applied directly in the sliding window scenario here. Certainly, the algorithm
has to be redesigned to use DataFrame (and/or Spark SQL) interface, it has
been noted that this is a very efficient way to execute window functions in
Spark}

\response{
We thank the reviewer for suggesting a better way of implementing 
the baseline algorithm (i.e., TRPM). However, this could
hardly be done after our investigations. The major reason is
that the current version of Spark-SQL (i.e., 1.6.2 just released in June)
does not support the User Defined Aggregate Function (UDAF) on window
function~\footnote{JIRA Spakr-8641: Native Spark Window Functions \url{https://issues.apache.org/jira/browse/SPARK-8641}}. Without UDAF, implementing
our Line Sweep algorithm (Algorithm 1) using Spark-SQL 
primitive aggregates (e.g., sum, avg, rank and nth-tile)
is beyond our abilities.

Further, we believe the performance boost on TRPM from Spark-SQL
is very limited. The reason is that our 
Line Sweep Mining algorithm (Algorithm 1) is not a 
properly reducible function. That is, the result of a 
smaller window could not be used to compute the result of a larger
window. In such a case, Spark-SQL has to process each
window independently in parallel, which is equivalent to our TRPM implementation.
%We agree that leveraging more advanced Spark features
%could further improve the performance of our solutions. 
%We do not heavily leverage these features because 
%we wish to focus on designing parallel GCMP mining algorithms.
%%to provide a general parallel framework which does not tie to Spark.
%Nevertheless, we are in progress in developing
%a better open-sourced version with Spark-tailored optimization. 
%In the revision, we try to add a DataFrame based TRPM solution 
%which aims to leverage the window functions from Spark-SQL.
%However, to the best of our efforts, we could not achieve this goal in a short time. 
%The major challenge is that current Spark do not support User Defined Aggregate Function (UDAF)
%in window functions~\footnote{JIRA Spakr-8641: Native Spark Window Functions \url{https://issues.apache.org/jira/browse/SPARK-8641}}. Without UDAF, implementing
%our Line sweep algorithm (Algorithm 1) using Spark primitives (e.g., sum, avg, rank and nth-tile)
%is nontrivial. Nonetheless, we expect less performance
%boost from TRPM even with the UDAF support. This is because the Line sweep algorithm is not
%properly reducible, where Spark system would fail to leverage the partial aggregates
%across multiple windows. As a result, it would be equivalent to our current TRPM
%solution where windows are processed independently in parallel.
%TRPM although can be modeled using window functions over data frame, the aggregates
%it uses (i.e., the Line Sweeping method in Algorithm 1) cannot be simply represented
%by Spark primitives (e.g., sum, avg, rank, nth-tile). We then resort to
%the Spark User Defined Aggregate Function (UDAF). However, in Spark 1.5.2, the UDAF
%does not support window function syntax. Even in the latest Spark 1.6.0 (released 
%last month), UDAF does not fit with window function. We confirm these information
%with Spark development team~\footnote{In Spark JIRA tickets, fully supporting UDAF is still under development}.
%
%Nevertheless, since our Algorithm 1 is not reducible, even with UDAF, TRPM could
%not boost too much 
%
%we expect
%even with UDAF support TRPM would not boost too much.  Spark system is 
%%hard to leverage the partial aggregates acrocess
%
%we can analytically expect even using UDAF, TRPM would
%not boost too much. This is because our Algorithm 1 is not reducible. That
%is we cannot merge or share partial aggregates across different windows. In summary,
%We wish to convince the reviewer that current TRPM implementation is reasonable. 
}

\emph{In particular, it would be great to provide the
difference in the number of partitions/splits, the amount of processing and
memory usage (i.e., vcore and memory seconds) between TRPM and SPARE}

\response{In our implementation, we take a fixed 486 partitions (Section 6)
for each RDD. The partition number is determined based on 
the parallel tuning guide by Cloudera~\footnote{Spark Performance Tuning \url{http://blog.cloudera.com/blog/2015/03/how-to-tune-your-apache-spark-jobs-part-1/}}.
To study the load balance, we revise Section 6.2.2 by comparing TRPM, SPARE with random 
task allocation and SPARE with best-fit task allocation (Section 5.3). We present
the summary of the execution time for each method on longest job, shortest job and standard
deviation of all jobs in Table 9. As the table show, SPARE has a better load balance (low deviation)
than SPARE-RD which indicates the effectiveness of the best-fit strategy. Further, TRPM
generally keeps a low deviation. This is because that TRPM always adopts the equal-size (i.e. $\eta$) partition.
Nevertheless, both the longest and shortest jobs in TRPM are higher than SPARE based solutions.

Next, we add Table 7 (Section 6.1) to compare
the resource usage (e.g., Vcore-seconds, Memory and Running Time) between SPARE and TRPM.
The table tells that both SPARE and TRPM are resource efficient.
In particular, the RDDs for both SPARE and TRPM only take less than 20\% of 
the available memory. This infers the potential of SPARE and TRPM in handling 
even larger trajectories with current system resources.
%
%include vcore-seconds 
%and RDD memory usage for both TRPM and SPARE. 
%As the table show, both TRPM and SPARE are resource efficient. 
%The maintained RDDs only takes less than 20\% of the entire memory. 
%We also add a clear description on partitions in Section 6.
}


\emph{A plot that breaks down the performance gain by each method would
be greatly appreciated by the readers.}

\response{We complement Figure 8 (Section 6.1) by including the breakdown cost of TRPM. 
In SPARE, star partition takes place in the Map-Shuffle phase while
Apriori enumeration takes place in the Reduce phase. As shown in the
new Figure 8, both star partition and apriori enumeration contribute to 
the final performance gain of SPARE. In particular, in the Map-Shuffle phase,
SPARE saves 56\%-73\% in time as compared to TRPM. This indicates that 
less data are transformed and shuffled in the star partition.
In the Reduce phase, SPARE saves 46\%-81\% time as compared to TRPM. This confirms 
the efficiency of our Apriori enumerator with various optimization (e.g., sequence simplification,
anti-monotonicity and forward closure checking).
}

\emph{Some choices of words may need to be reconsidered: for example, "a bunch
of" might not be appropriate in a technical paper.}

\response{We thank the reviewer for the correction. We have changed these unprofessional terms.
}


\emph{References to star partitioning and apriori pruning are missing. Though these
are well known, they need to clearly cited. At least the following reference is
missing:}

\response{We add the corresponding references in Section 5.1 and Section 5.2.}

\emph{In "In contrast, when utilizing the multicore
environment, SPAREP achieves 7 times speedup and SPARES achieves 10 times speedup.", was "multicore"
referring to the use of all 16 cores in one of your node? The specification of the machine was not clear.}

\response{ We use all cores in the single node to conduct the experiment. We revise the description in Section 6.2.3.
} 


\emph{The computation of "eta" was slightly different than that in the paper}

\response{
We have updated the GitHub repository to rectify the typo in the equation.}