\section{Responses to Reviewer 3}
\textbf{3.1:} \emph{Though the implementation references Spark as the implementation
platform (as it also reflects in github repo provided), the algorithm design is
mostly limited to MapReduce, aka only Hadoop, which is a very small subset of
Spark. This may have a negative impact on the baseline implementation.
Particularly, recent releases of Spark have introduced window functions that can
be applied directly in the sliding window scenario here. Certainly, the algorithm
has to be redesigned to use DataFrame (and/or Spark SQL) interface, it has
been noted that this is a very efficient way to execute window functions in
Spark.}

\textbf{Response:}
We thank the reviewer for suggesting a better way of implementing 
the baseline algorithm (i.e., TRPM). By utilizing window functions,
TRPM could be rewritten as in the following pseudo-code:
\begin{equation*}
\begin{aligned}
 \mathtt{WindowSpec}\ ws := \mathtt{Window}&.\mathtt{orderBy}('time') \\
 				&.\mathtt{rangeBetween}(0, \eta); \\
 \mathtt{Pattern}\ pt :=  LineSweep&('clusters').\mathtt{over}(ws); 
\end{aligned}
\end{equation*}

However, the algorithm is challenging
to be implemented after our investigations. The major reason is
that, in current version of Spark-SQL~\footnote{Both the stable Spark 1.5.2 and the newly released Spark 1.6.2}, 
User Defined Aggregate Function (UDAF) on window operator is not supported~\footnote{JIRA Spakr-8641: Native Spark Window Functions \url{https://issues.apache.org/jira/browse/SPARK-8641}}. 
Without UDAF, we cannot attach the $\mathtt{over}$ keyword to
the Line Sweep algorithm. A workaround is to implement the Line Sweep using only Spark-SQL primitive aggregate 
functions (e.g., sum, avg, rank and nth-tile). But it
is beyond our abilities.
%
%we cannot integrate the Line Sweep algorithm with window functions 
%(i.e., with the $\mathtt{over}$ keyword). On the other hand,
%implementing the Line Sweep algorithm (Algorithm 1) using Spark-SQL 
%primitive aggregates (e.g., sum, avg, rank and nth-tile)
%is beyond our abilities.

On the other hand, we believe the performance boost on TRPM from Spark-SQL
would be very limited. The reason is that our 
Line Sweep algorithm (Algorithm 1) is not a 
distributive function. 
That is the result over an input cannot be constructed from
the result over a subset of the input. This indicates 
that no partial results can be shared among all windows.
In such a case, Spark-SQL has to process each
window independently in parallel, which is equivalent to our TRPM implementation.
%We agree that leveraging more advanced Spark features
%could further improve the performance of our solutions. 
%We do not heavily leverage these features because 
%we wish to focus on designing parallel GCMP mining algorithms.
%%to provide a general parallel framework which does not tie to Spark.
%Nevertheless, we are in progress in developing
%a better open-sourced version with Spark-tailored optimization. 
%In the revision, we try to add a DataFrame based TRPM solution 
%which aims to leverage the window functions from Spark-SQL.
%However, to the best of our efforts, we could not achieve this goal in a short time. 
%The major challenge is that current Spark do not support User Defined Aggregate Function (UDAF)
%in window functions~\footnote{JIRA Spakr-8641: Native Spark Window Functions \url{https://issues.apache.org/jira/browse/SPARK-8641}}. Without UDAF, implementing
%our Line sweep algorithm (Algorithm 1) using Spark primitives (e.g., sum, avg, rank and nth-tile)
%is nontrivial. Nonetheless, we expect less performance
%boost from TRPM even with the UDAF support. This is because the Line sweep algorithm is not
%properly reducible, where Spark system would fail to leverage the partial aggregates
%across multiple windows. As a result, it would be equivalent to our current TRPM
%solution where windows are processed independently in parallel.
%TRPM although can be modeled using window functions over data frame, the aggregates
%it uses (i.e., the Line Sweeping method in Algorithm 1) cannot be simply represented
%by Spark primitives (e.g., sum, avg, rank, nth-tile). We then resort to
%the Spark User Defined Aggregate Function (UDAF). However, in Spark 1.5.2, the UDAF
%does not support window function syntax. Even in the latest Spark 1.6.0 (released 
%last month), UDAF does not fit with window function. We confirm these information
%with Spark development team~\footnote{In Spark JIRA tickets, fully supporting UDAF is still under development}.
%
%Nevertheless, since our Algorithm 1 is not reducible, even with UDAF, TRPM could
%not boost too much 
%
%we expect
%even with UDAF support TRPM would not boost too much.  Spark system is 
%%hard to leverage the partial aggregates acrocess
%
%we can analytically expect even using UDAF, TRPM would
%not boost too much. This is because our Algorithm 1 is not reducible. That
%is we cannot merge or share partial aggregates across different windows. In summary,
%We wish to convince the reviewer that current TRPM implementation is reasonable. 


\textbf{3.2:} \emph{Most implementations
on Hadoop and Spark are very sensitive to data partitions, i.e. prone to data
skewing issues. It seems that the (starpartition)
implementation does not take
this into account, and only use the default partitioning. It would be very helpful
to have a discussion on this topic.}

\textbf{Response:}  We study the skewness by examining the load balance of each algorithm.
We revise Section 6.2.2 by comparing SPARE with different data partition strategies.
In particular, we compare SPARE which adopts a best-fit strategy (Section 5.3) and SPARE-RD
which adopts the default random partition strategy.
We present the summary of the execution time for each method on the longest job, the shortest job and standard
deviation of all jobs in Table 9. As the table shows, SPARE has a better load balance (low deviation)
than SPARE-RD which indicates the effectiveness of the best-fit strategy. 
%Further, TRPM generally keeps a low deviation. This is because that TRPM always adopts the equal-size (i.e. $\eta$) partition. Nevertheless, both the longest and shortest jobs in TRPM are higher than SPARE based solutions.


%We revise Section 6.2.2 by comparing TRPM, SPARE with random 
%task allocation and SPARE with best-fit task allocation (Section 5.3). We present
%the summary of the execution time for each method on longest job, shortest job and standard
%deviation of all jobs in Table 9. As the table show, SPARE has a better load balance (low deviation)
%than SPARE-RD which indicates the effectiveness of the best-fit strategy. Further, TRPM
%generally keeps a low deviation. This is because that TRPM always adopts the equal-size (i.e. $\eta$) partition.
%Nevertheless, both the longest and shortest jobs in TRPM are higher than SPARE based solutions.


\textbf{3.3:} \emph{In particular, it would be great to provide the
difference in the number of partitions/splits, the amount of processing and
memory usage (i.e., vcore and memory seconds) between TRPM and SPARE}

\textbf{Response:} In our implementation, we take a fixed 486 partitions (Section 6)
for each RDD. The partition number is determined according to
the parallel tuning guide by Cloudera~\footnote{Spark Performance Tuning \url{http://blog.cloudera.com/blog/2015/03/how-to-tune-your-apache-spark-jobs-part-1/}}: there are  $3\times 54=162$ cores and each core is able to processes 3 partitions~\footnote{This number may differ for other CPU models, and can be determined empirically.}
by multithreading.

Next, we add Table 7 (Section 6.1) to compare
the resource usage (e.g., Vcore-seconds, Memory and Running Time) between SPARE and TRPM.
The table tells that both SPARE and TRPM are resource efficient.
In particular, the RDDs for both SPARE and TRPM only take less than 20\% of 
the available memory. This infers the potential of SPARE and TRPM in handling 
even larger trajectories with our current cluster resources.



\textbf{3.4:} \emph{A plot that breaks down the performance gain by each method would
be greatly appreciated by the readers.}

\textbf{Response:} We include the analysis of
the breakdown cost of SPARE by taking TRPM as
a reference in Figure 8. 
SPARE and TRPM algorithms are of the similar structure: partitioning (Star partition v.s. $\eta$-replicate
partition) is in the map-shuffle phase, and  mining (Apriori Enumeration v.s. Line Sweep) 
is in the reduce phase. In our experiment, we treat the execution time
for each phase as the cost of the corresponding methods.

As shown in Figure 8,
both star partition and apriori enumeration contribute to 
the final performance gain of SPARE. In particular, in the Map-Shuffle phase,
SPARE saves 56\%-73\% in time as compared to TRPM. This indicates that 
less data are transformed and shuffled in the Star partition.
In the Reduce phase, SPARE saves 46\%-81\% time as compared to TRPM. This confirms 
the efficiency of our Apriori enumerator.

% with various optimization (e.g., sequence simplification, anti-monotonicity and forward closure checking).


\textbf{3.5} \emph{Some choices of words may need to be reconsidered: for example, ``a bunch
of" might not be appropriate in a technical paper.}

\textbf{Response:} We have changed these unprofessional terms.



\textbf{3.6} \emph{References to star partitioning and apriori pruning are missing. Though these
are well known, they need to  be clearly cited.}

\textbf{Response:} We add the corresponding references in Section 5.1 and Section 5.2 when we
describing our algorithms.

\textbf{3.7} \emph{In ``In contrast, when utilizing the multicore
environment, SPAREP achieves 7 times speedup and SPARES achieves 10 times speedup.", was ``multicore"
referring to the use of all 16 cores in one of your node? The specification of the machine was not clear.}

\textbf{Response:} We use all cores in the single node (i.e., 4 executors each with 3 cores) to conduct the experiment. We revise the description in Section 6.2.3.



\textbf{3.8} \emph{The computation of "eta" was slightly different than that in the paper}

\textbf{Response:} We have updated the GitHub repository to rectify the typo in the equation.