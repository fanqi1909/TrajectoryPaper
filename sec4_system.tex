\section{Overview of Mining GCMP in Parallel}
\label{sec:system_overview}
We adapt the MapReduce paradigm for designing
a parallel solution of mining GCMP. In this section,
we briefly cover the preliminary on MapReduce and then describe
the overview of our framework in mining GCMP.

\subsection{Preliminary on MapReduce}
MapReduce (MR) was formally proposed by Dean et. al.~\cite{dean2008mapreduce}
and has subsequently implemented by many open source systems. Those systems
provide handy APIs with fualt tolerances and are popularly
used as large-scale data processing platforms. 
% become a ubiquitous parallel platform for large-scaled data processing. 
%Current open source MapReduce systems provide handy programming APIs with fault tolerances
%in backends. Such systems include Hadoop, Shark and Spark to name a few.
In simple words, there are two conceptual types of computing nodes in MR,
namely the \emph{mapper}s and the \emph{reducer}s. The execution of a MR 
algorithm consists of three stages: First, input data
are partitioned and read by a \emph{map} function on each mapper. Then, mappers
emit key-value pairs which are \emph{shuffle}d over the network to reducers. Finally,
reducers process received data using a \emph{reduce} function then write the
outputs. 

Despite the simpleness of the paradigm, there are two concerns raised in designing MR algorithms
First, since it requires reducers to be independent, partitioning  
data to fit the independence could be challenging. Second, since the \emph{shuffle}
stage requires network access, the data been shuffled should be minimized. 
We take these concerns in consideration in designing our solutions for mining GCMP.
%We would bare this concerns in designing our GCMP mining algorithm.
%a natural
%problem is how to partition the data in map and shuffle phase so that reducers 
%achieve the independence.
%
%A critical question in designing a MapReduce algorithm is how to make 
%data partitions during the shuffle stage. In fact, a MapReduce algorithm 
%requires data processed at each reducer to be independent.
%
%Since the \emph{shuffle} stage needs to transfer data over network, 
%an important attention to pay during designing MapReduce an algorithm is 
%to minimize the shuffle amounts and shuffle counts. 

\subsection{MapReduce Processing for Mining GCMP}
Our GCMP mining process consists of two MR jobs. The two jobs
contains four MR stages as illustrated in Figure~\ref{fig:overview}.
The first MR job is to cluster objects at each snapshot (i.e.,
computing $\forall t, o, C_t(o)$). As shown in Figures~\ref{fig:overview}(a)-(b),
trajectories are first reorganized by timestamps in map phase and object locations
at the same timestamps form a cluster. The clustering takes place in reduce phase, where
each snapshot is processed independently. The second MR job is to mine GCMPs from 
clusters at each snapshot. We design two MR algorithms (i.e., TRM and SPM) for 
the purpose of comparison. Both the two algorithms would partition snapshots in map phase
(as in Figure~\ref{fig:overview}(c))
and mine GCMPs from each partition in reduce phase(as in Figure~\ref{fig:overview}(d)).

% corresponds to Figures~\ref{fig:overview}(a)-(b). The objective
%of the first job is to cluster trajectories based on snapshots (i.e.,).
%As illustrated in (a), input trajectories are read in by mappers 
%and are sprinkled into $\langle t,o \rangle$ pairs. In (b), objects with the same timestamp
%form a snapshot. Then, a user defined clustering method is applied on the objects
%in each snapshot in reducers. Between step (a) and (b), a shuffle is necessary. 
%The second
%MapReduce job corresponds to Figures~\ref{fig:overview} (c)-(d). The objective 
%of the second job is to mine the GCMP from the snapshots computed in the first job. We design
%two approaches (to be described shortly)for mining GCMP in parallel. In overview, as shown in (c), snapshots
%are first fed to mappers and different partition strategies may be selected to create
%partitions among snapshots. In (d), the partitions are then send to reducers to mine GCMP.
%The final computed results are then outputted to the end users. 

Although we need two MR jobs to complete the GCMP mining task, 
it is easy to pipeline the two jobs to exploit data locality.
Specifically, the reducer output at step (b) can be directly reused 
as the input to mappers at step (c). Therefore we do not need to 
transfer data from (b) to (c). Modern MR platforms, especially Spark, have
already supported such a kind of pipeline.

\begin{figure} [t]
\center
\includegraphics[width=0.5\textwidth]{system_layout.eps}
\caption{System flow of mining GCMP. (a)(b) correspond to the first MR job which compute the clusters at each snapshot; 
(c)(d) correspond to the second MR jobs which mines GCMP in parrallel.}
\label{fig:overview}
\end{figure}

We note that the first MR job is easy to design since each reducer only
needs one snapshot for clustering.
In contrast, it is challenging to design the second job. 
This is because valid patterns may spray across multiple snapshots, where inappropriate partitioning
of snapshots may fail to discover certain valid patterns.
Formally, a reasonable partition strategy 
needs to meet the following requirements: (a) the resulted partitions need
to preserve enough information so that real patterns can be discovered in the reduce phase. 
(b) the resulted partitions need to ensure that
the patterns discovered in the reduce phase are valid patterns so that
no further verification is required. We formalize the two 
properties as \emph{completeness} and \emph{soundness} as follows:

\begin{definition}[Completeness and Soundness]
Let a partition method $\mathbb{P}$ partitions original trajectories $Tr$ into multiple parts, $Par_1,...,Par_m$. $\mathbb{P}$ is complete if for every pattern $P$ that is valid in $Tr$, $\exists Par_i$ such that $P$ is valid in $Par_i$. $\mathbb{P}$ is sound if for all patterns $P$ that is valid in any $Par_i$, it is also valid in $TR$.
\end{definition}
The completeness ensures that no true patterns are missed out. 
The soundness ensures that no false patterns are reported. 
If a partition method is both sound and complete, then it can be used
in the second MR job to facilitate GCMP mining.

Apparently, replicating the entire trajectories to each 
partition meets the \emph{soundness} and \emph{completeness} requirements. 
However, it burdens the network shuffle and limits the parallelism. 
Our objective is thus to design a complete and sound partition method that minimize the network shuffles.
In the following sections, we describe a naive \emph{temporal-based} partition-and-mining method called \emph{Temporal Replication and Mining}(TRM) towards a parallel solution of GCMP mining. Then,
we present a novel \emph{object-based} partition-and-mining method
called \emph{Star Partition and Mining} (SPM) which resolves
the deficiencies of TRM method.

%Depends on the pattern mining strategy, the size of shuffle and replication may differ. We show that if the pattern mining strategy is not selected carefully, the second stage will suffer from large data replication and shuffling.


%
%In particular, we need
%the number of partitions to be reasonably large in order to achieve
%good parallelism in stage (d). Meanwhile, we need to ensure a partition 
%of snapshots containing enough information so that patterns discovered 
%in stage (d) are complete.  Furthermore, we have to ensure that the
%patterns that discovered in stage (d) are real patterns.

%However, there are two conflicting requirements that restrain us from 
%designing a simple and effective solution. 
%On one hand, we need to partition snapshots in stage (c) as many as possible 
%so that good parallelism could be achieved in stage (d). 
%On the other hand, in stage (d), 
%we require each partition containing enough information in order to discover
%proper GCMP patterns. This prevents us from arbitrarily partition snapshots in
%stage (d). To design a working algorithm, find the minimum size of partition is
%crucial. In the following section, we first design a naive approach that
%partitions snapshots by allowing overlapping among partitions. Such a partition strategy
%although has overhead in shuffling, it corrects find GCMPs in parallel in stage (d).
%In order to discover all patterns, we require a partitioning method to be \emph{complete} and \emph{Sound}. 

%\section{Temporal Replication and Mining}
%
%
%%It is notable that some patterns 
%%may be very long in duration (e.g., when discovering \emph{swarm}) or some patterns 
%%may contain a large set of objects (e.g., when discover \emph{group} patterns with small $L$).
%%Therefore, we require the size of partition to be large enough. Reaching the
%%balance between the two requirement is crucial. In the following sections, we will
%%describe a naive partition scheme which meets the two requirements.
%
%%Our parallel solution incurs two rounds of shuffles. The first round of shuffle happens in step (a). This
%%is because that the trajectories stored in HDFS needs to be regrouped to form snapshots. 
%%The second round of shuffle occurs in step(d). This is because that 
%% the clusters need to be regrouped so that subsequent pattern detection can be performed.
%% Despite the simpleness of our parallel solution, it is actually nontrivial to work out a scalable solution.
%% The major challenge lies in the step (d) and (e). 
%
%
%%\section{Preliminary on Apache Spark}
%%\label{sec:system_overview}
%%Apache Spark is an open sourced modern parallel processing platform
%%based on Resilient-Distributed-Dataset (RDD). Each RDD is able to take an action (including Map, Reduce, GroupByKey etc.) and then transforms to a new RDD. Algorithms in Spark is implemented by supplying various actions to an input RDD. Each RDD in Spark has a pre-defined and fixed number of partitions, where each partition forms a task assigned to an executor.
%%
%%Compared to MapReduce, Spark 
%%utilizes distributed memory to cache the programming data modeled as RDDs, which brings computational benefits~\cite{shi2015clash}. In order to fully utilize the memory, Spark creates one thread for each task and then multiple tasks belonging to the same JVM share the available memory.
%Due to threading, there is a paradigm shift from MapReduce to Spark. In MapReduce, the cost of starting a task is expensive, as each task in MapReduce requires a dedicated JVM process. While in Spark, the cost of starting a task is negligible since each task is done by a thread.  Therefore in Spark, we may create many more parallel tasks than in MapReduce before reaching to the system limitation.


%\section{Mining Generalized Co-moving Pattern}
%\label{sec:solution}
%There are two stages in mining GCMP. The first stage is to cluster objects in each snapshots. Then the seconds stage is to mine patterns from clusters in parallel. The overview of the parallel approach is 
%shown in Figure~\ref{fig:overview}. As shown, trajectory data is initially stored in HDFS. The first stage is to 
%read trajectories and cluster objects in the same snapshots. Since the objects at each snapshots are independent, this stage can be easily performed in parallel. The first stage involves network IO. Afterwards, the clusters in each snapshots are shuffled and replicated to make various partitions.



